import cl100kBase from 'gpt-tokenizer';

var Tokenizers = /*#__PURE__*/ function(Tokenizers) {
    Tokenizers["CL100K_BASE"] = "cl100k_base";
    return Tokenizers;
}({});

class TokenizerSingleton {
    #defaultTokenizer;
    constructor(){
        this.#defaultTokenizer = {
            encode: (text)=>{
                return new Uint32Array(cl100kBase.encode(text));
            },
            decode: (tokens)=>{
                return cl100kBase.decode(tokens);
            }
        };
    }
    tokenizer(encoding) {
        if (encoding && encoding !== Tokenizers.CL100K_BASE) {
            throw new Error(`Tokenizer encoding ${encoding} not yet supported`);
        }
        return this.#defaultTokenizer;
    }
}
const tokenizers = new TokenizerSingleton();

export { Tokenizers, tokenizers };
