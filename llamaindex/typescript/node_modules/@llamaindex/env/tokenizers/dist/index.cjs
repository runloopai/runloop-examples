Object.defineProperty(exports, '__esModule', { value: true });

var cl100kBase = require('gpt-tokenizer');

function _interopDefault (e) { return e && e.__esModule ? e : { default: e }; }

var cl100kBase__default = /*#__PURE__*/_interopDefault(cl100kBase);

var Tokenizers = /*#__PURE__*/ function(Tokenizers) {
    Tokenizers["CL100K_BASE"] = "cl100k_base";
    return Tokenizers;
}({});

class TokenizerSingleton {
    #defaultTokenizer;
    constructor(){
        this.#defaultTokenizer = {
            encode: (text)=>{
                return new Uint32Array(cl100kBase__default.default.encode(text));
            },
            decode: (tokens)=>{
                return cl100kBase__default.default.decode(tokens);
            }
        };
    }
    tokenizer(encoding) {
        if (encoding && encoding !== Tokenizers.CL100K_BASE) {
            throw new Error(`Tokenizer encoding ${encoding} not yet supported`);
        }
        return this.#defaultTokenizer;
    }
}
const tokenizers = new TokenizerSingleton();

exports.Tokenizers = Tokenizers;
exports.tokenizers = tokenizers;
